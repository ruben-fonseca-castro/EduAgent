<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Probability for Computer Science: Foundations and Applications — Astra</title>

  <!-- CDN Dependencies -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.plot.ly/plotly-2.35.2.min.js" charset="utf-8"></script>
  <script>
    MathJax = {
      tex: { inlineMath: [['$', '$']], displayMath: [['$$', '$$']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-svg.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.1/dist/mermaid.min.js"></script>

  <style>
    /* Reading progress bar */
    #progress-bar {
      position: fixed;
      top: 0; left: 0;
      height: 3px;
      background: linear-gradient(90deg, #FFCB05, #D50032);
      z-index: 100;
      transition: width 0.1s linear;
    }

    /* Callout boxes */
    .callout {
      border-left: 4px solid;
      padding: 1rem 1.25rem;
      margin: 1.25rem 0;
      border-radius: 0 0.5rem 0.5rem 0;
    }
    .callout-info {
      background: #eff6ff;
      border-color: #00274C;
    }
    .callout-exercise {
      background: #fefce8;
      border-color: #FFCB05;
    }
    .callout-warning {
      background: #fff1f2;
      border-color: #D50032;
    }

    /* Code blocks */
    pre {
      background: #1e1e2e;
      color: #cdd6f4;
      padding: 1rem 1.25rem;
      border-radius: 0.5rem;
      overflow-x: auto;
      font-size: 0.875rem;
      line-height: 1.6;
    }
    code:not(pre code) {
      background: #f3f4f6;
      padding: 0.1rem 0.3rem;
      border-radius: 0.25rem;
      font-size: 0.875em;
      color: #00274C;
    }

    /* Mermaid diagrams */
    .mermaid {
      background: #f9fafb;
      border-radius: 0.5rem;
      padding: 1rem;
      margin: 1rem 0;
      text-align: center;
    }

    /* Active section nav */
    .nav-link.active {
      background: #FFCB0520;
      color: #00274C;
      font-weight: 600;
      border-left: 3px solid #FFCB05;
    }

    /* Smooth scroll */
    html { scroll-behavior: smooth; }

    /* Section cards */
    .section-card {
      scroll-margin-top: 5rem;
    }

    /* MathJax display */
    .mathjax-equation {
      text-align: center;
      margin: 1.5rem 0;
      overflow-x: auto;
    }

    /* Prose styling */
    .prose h3 { font-size: 1.125rem; font-weight: 700; margin-top: 1.25rem; margin-bottom: 0.5rem; color: #00274C; }
    .prose h4 { font-size: 1rem; font-weight: 600; margin-top: 1rem; margin-bottom: 0.25rem; color: #1e293b; }
    .prose p { margin-bottom: 0.75rem; line-height: 1.7; }
    .prose ul, .prose ol { margin-left: 1.5rem; margin-bottom: 0.75rem; }
    .prose li { margin-bottom: 0.25rem; line-height: 1.6; }
    .prose strong { color: #00274C; }
    .prose blockquote { border-left: 3px solid #FFCB05; padding-left: 1rem; margin: 1rem 0; color: #475569; font-style: italic; }
  </style>
</head>
<body class="bg-gray-50 text-gray-900" onscroll="updateProgress()">

  <!-- Reading progress bar -->
  <div id="progress-bar" style="width: 0%"></div>

  <!-- Sticky Header -->
  <header class="sticky top-0 z-50 border-b border-gray-200 shadow-sm" style="background: #00274C;">
    <div class="max-w-7xl mx-auto px-4 py-3 flex items-center justify-between">
      <div>
        <div class="flex items-center gap-2">
          <span class="text-2xl">&#x2B50;</span>
          <h1 class="text-xl font-bold text-white">Probability for Computer Science: Foundations and Applications</h1>
        </div>
        <div class="flex items-center gap-3 mt-0.5 text-sm" style="color: #FFCB05;">
          <span>Computer Science</span>
          <span style="opacity: 0.5;">&#x2022;</span>
          <span>Undergraduate</span>
          <span style="opacity: 0.5;">&#x2022;</span>
          <span>25 min</span>
          <span style="opacity: 0.5;">&#x2022;</span>
          <span>Student</span>
        </div>
      </div>
      <div class="text-right text-xs" style="color: rgba(255,255,255,0.6);">
        <div>Generated by Astra</div>
        <div>February 20, 2026</div>
      </div>
    </div>
  </header>

  <!-- Main Layout -->
  <div class="max-w-7xl mx-auto px-4 py-6">
    <div class="grid grid-cols-1 lg:grid-cols-4 gap-6">

      <!-- Sidebar -->
      <aside class="lg:col-span-1">
        <div class="sticky top-24 space-y-4">

          <!-- Learning Objectives -->
          <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-4">
            <h2 class="font-semibold text-gray-800 mb-3 flex items-center gap-2">
              <span>&#x1F3AF;</span> Learning Objectives
            </h2>
            <ul class="space-y-2">
              
              <li class="flex gap-2 text-sm text-gray-600">
                <span class="mt-0.5 flex-shrink-0" style="color: #FFCB05;">&#x2713;</span>
                <span>Understand the axioms of probability and conditional probability.</span>
              </li>
              
              <li class="flex gap-2 text-sm text-gray-600">
                <span class="mt-0.5 flex-shrink-0" style="color: #FFCB05;">&#x2713;</span>
                <span>Apply Bayes’ Theorem to real-world computer science problems.</span>
              </li>
              
              <li class="flex gap-2 text-sm text-gray-600">
                <span class="mt-0.5 flex-shrink-0" style="color: #FFCB05;">&#x2713;</span>
                <span>Analyze discrete random variables and their distributions.</span>
              </li>
              
              <li class="flex gap-2 text-sm text-gray-600">
                <span class="mt-0.5 flex-shrink-0" style="color: #FFCB05;">&#x2713;</span>
                <span>Implement a Naive Bayes classifier in Python.</span>
              </li>
              
            </ul>
          </div>

          <!-- Section Navigation -->
          <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-4">
            <h2 class="font-semibold text-gray-800 mb-3 flex items-center gap-2">
              <span>&#x1F4CB;</span> Sections
            </h2>
            <nav class="space-y-1">
              
              <a href="#section-0"
                 class="nav-link block px-3 py-2 rounded-lg text-sm text-gray-600 hover:bg-gray-100 transition-colors"
                 data-section="0">
                <span class="text-gray-400 mr-1">1.</span>
                Axioms of Probability and Conditional Probability
              </a>
              
              <a href="#section-1"
                 class="nav-link block px-3 py-2 rounded-lg text-sm text-gray-600 hover:bg-gray-100 transition-colors"
                 data-section="1">
                <span class="text-gray-400 mr-1">2.</span>
                Bayes’ Theorem in Spam Filtering
              </a>
              
              <a href="#section-2"
                 class="nav-link block px-3 py-2 rounded-lg text-sm text-gray-600 hover:bg-gray-100 transition-colors"
                 data-section="2">
                <span class="text-gray-400 mr-1">3.</span>
                Discrete Random Variables and the Binomial Distribution
              </a>
              
              <a href="#section-3"
                 class="nav-link block px-3 py-2 rounded-lg text-sm text-gray-600 hover:bg-gray-100 transition-colors"
                 data-section="3">
                <span class="text-gray-400 mr-1">4.</span>
                Naive Bayes Classifier Implementation
              </a>
              
            </nav>
          </div>

          <!-- Progress -->
          <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-4">
            <h2 class="font-semibold text-gray-800 mb-2 flex items-center gap-2">
              <span>&#x1F4CA;</span> Your Progress
            </h2>
            <div class="bg-gray-200 rounded-full h-2">
              <div id="lesson-progress" class="h-2 rounded-full transition-all" style="width: 0%; background: linear-gradient(90deg, #FFCB05, #D50032);"></div>
            </div>
            <p class="text-xs text-gray-500 mt-1.5">
              <span id="progress-pct">0</span>% complete
            </p>
          </div>

        </div>
      </aside>

      <!-- Content Area -->
      <main class="lg:col-span-3 space-y-6">

        
        <div id="section-0" class="section-card bg-white rounded-xl shadow-sm border border-gray-200 p-6">

          <!-- Section Header -->
          <div class="flex items-center gap-3 mb-4 pb-3 border-b border-gray-100">
            <span class="flex-shrink-0 w-8 h-8 rounded-lg flex items-center justify-center text-sm font-bold text-white" style="background: #00274C;">
              1
            </span>
            <div>
              <h2 class="text-xl font-bold text-gray-900">Axioms of Probability and Conditional Probability</h2>
              
              <span class="text-xs text-gray-400 uppercase tracking-wide">TEXT</span>
              
            </div>
          </div>

          <!-- Section Content -->
          <div class="prose max-w-none">
            <h2>Axioms of Probability and Conditional Probability</h2>
<p>Probability theory is built upon three fundamental axioms: non-negativity, normalization, and additivity. These axioms provide a mathematical framework for quantifying uncertainty. The <strong>non-negativity axiom</strong> states that the probability of an event cannot be negative: $P(A) \geq 0$. The <strong>normalization axiom</strong> ensures that the probability of the entire sample space is 1: $P(S) = 1$. The <strong>additivity axiom</strong> allows us to compute the probability of the union of disjoint events: $P(A \cup B) = P(A) + P(B)$ if $A$ and $B$ are mutually exclusive.</p>

<p><strong>Conditional probability</strong> measures the likelihood of an event occurring given that another event has already occurred. It is defined as:  
$$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$  
where $P(B) > 0$. This concept is crucial for understanding dependencies between events.</p>

<div class="callout callout-info">
<p><strong>Key Concept:</strong> Bayes’ Theorem is derived directly from the definition of conditional probability. It states:  
$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$  
This theorem is essential for inverting probabilities and is widely used in computer science, particularly in machine learning and data analysis.</p>
</div>
          </div>

          <!-- Figures for this section -->
          

        </div>
        
        <div id="section-1" class="section-card bg-white rounded-xl shadow-sm border border-gray-200 p-6">

          <!-- Section Header -->
          <div class="flex items-center gap-3 mb-4 pb-3 border-b border-gray-100">
            <span class="flex-shrink-0 w-8 h-8 rounded-lg flex items-center justify-center text-sm font-bold text-white" style="background: #00274C;">
              2
            </span>
            <div>
              <h2 class="text-xl font-bold text-gray-900">Bayes’ Theorem in Spam Filtering</h2>
              
              <span class="text-xs text-gray-400 uppercase tracking-wide">TEXT + EQUATION</span>
              
            </div>
          </div>

          <!-- Section Content -->
          <div class="prose max-w-none">
            <h2>Bayes’ Theorem in Spam Filtering</h2>
<p>Spam filtering is a classic application of Bayes’ Theorem. The goal is to classify an email as either spam ($S$) or not spam ($\neg S$) based on its content. Bayes’ Theorem is applied as follows:  
$$ P(S|W) = \frac{P(W|S)P(S)}{P(W)} $$  
where $W$ represents the words in the email. The classifier calculates the probability of the email being spam given the observed words and compares it to the probability of it being non-spam.</p>

<p><strong>Naive Bayes classifiers</strong> simplify this problem by assuming independence between features (words). This "naive" assumption reduces computational complexity, making the classifier efficient even for large datasets. The probability of a word given the class is estimated from training data, and the prior probabilities $P(S)$ and $P(\neg S)$ are also learned from the data.</p>

<p>Mathematically, the Naive Bayes classifier computes:  
$$ P(S|W) = \frac{P(W_1|S)P(W_2|S)\dots P(W_n|S)P(S)}{P(W)} $$  
The email is classified as spam if $P(S|W) > P(\neg S|W)$.</p>
          </div>

          <!-- Figures for this section -->
          

        </div>
        
        <div id="section-2" class="section-card bg-white rounded-xl shadow-sm border border-gray-200 p-6">

          <!-- Section Header -->
          <div class="flex items-center gap-3 mb-4 pb-3 border-b border-gray-100">
            <span class="flex-shrink-0 w-8 h-8 rounded-lg flex items-center justify-center text-sm font-bold text-white" style="background: #00274C;">
              3
            </span>
            <div>
              <h2 class="text-xl font-bold text-gray-900">Discrete Random Variables and the Binomial Distribution</h2>
              
              <span class="text-xs text-gray-400 uppercase tracking-wide">TEXT + EQUATION</span>
              
            </div>
          </div>

          <!-- Section Content -->
          <div class="prose max-w-none">
            <h2>Discrete Random Variables and the Binomial Distribution</h2>
<p>A <strong>discrete random variable</strong> is one that can take on a finite or countable number of distinct values. Its probability distribution is described by a <strong>probability mass function (PMF)</strong>, which assigns probabilities to each possible value of the variable. For example, if $X$ is a discrete random variable, its PMF is given by $P(X = k)$.</p>

<p>The <strong>Binomial distribution</strong> is a common discrete distribution used to model the number of successes in $n$ independent Bernoulli trials, each with success probability $p$. The PMF of a Binomial random variable $X$ is:  
$$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$  
where $\binom{n}{k}$ is the binomial coefficient, representing the number of ways to choose $k$ successes from $n$ trials.</p>

<p>This distribution is widely used in computer science for tasks like estimating the reliability of systems or modeling binary outcomes in data analysis.</p>
          </div>

          <!-- Figures for this section -->
          

        </div>
        
        <div id="section-3" class="section-card bg-white rounded-xl shadow-sm border border-gray-200 p-6">

          <!-- Section Header -->
          <div class="flex items-center gap-3 mb-4 pb-3 border-b border-gray-100">
            <span class="flex-shrink-0 w-8 h-8 rounded-lg flex items-center justify-center text-sm font-bold text-white" style="background: #00274C;">
              4
            </span>
            <div>
              <h2 class="text-xl font-bold text-gray-900">Naive Bayes Classifier Implementation</h2>
              
              <span class="text-xs text-gray-400 uppercase tracking-wide">CODE</span>
              
            </div>
          </div>

          <!-- Section Content -->
          <div class="prose max-w-none">
            <h2>Naive Bayes Classifier Implementation</h2>
<p>Below is a Python implementation of a Naive Bayes classifier for a simple text classification task. The code uses the `scikit-learn` library to train a classifier on a dataset of text documents.</p>

<pre><code class="python">
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample dataset
texts = ["I love this product", "This is terrible", "Great service!", "Worst experience ever"]
labels = ["positive", "negative", "positive", "negative"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.5, random_state=42)

# Vectorize text data
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Train Naive Bayes classifier
classifier = MultinomialNB()
classifier.fit(X_train_counts, y_train)

# Predict and evaluate
y_pred = classifier.predict(X_test_counts)
print("Accuracy:", accuracy_score(y_test, y_pred))
</code></pre>

<p><strong>Exercise:</strong> Modify the code to handle a different dataset, such as the 20 Newsgroups dataset, by importing it from `sklearn.datasets` and adjusting the preprocessing steps accordingly.</p>
          </div>

          <!-- Figures for this section -->
          

        </div>
        

        <!-- Completion card -->
        <div class="rounded-xl border p-6 text-center" style="background: linear-gradient(135deg, #00274C08, #FFCB0515); border-color: #FFCB05;">
          <div class="text-4xl mb-3">&#x1F389;</div>
          <h2 class="text-xl font-bold mb-2" style="color: #00274C;">Lesson Complete!</h2>
          <p style="color: #00274C;">You've finished <strong>Probability for Computer Science: Foundations and Applications</strong>.</p>
          <p class="text-sm mt-2" style="color: #00274Caa;">
            Review your notes, then start teaching to solidify your understanding.
          </p>
          <button onclick="window.parent.postMessage({type:'LESSON_COMPLETE'},'*')"
                  class="mt-4 inline-block text-white px-6 py-3 rounded-xl font-semibold hover:opacity-90 transition-opacity cursor-pointer"
                  style="background: linear-gradient(135deg, #D50032, #00274C);">
            I'm Ready to Teach &rarr;
          </button>
        </div>

      </main>
    </div>
  </div>

  <!-- Plotly Figure Data + Interactivity -->
  <script>
    // Embedded figure data (all_figures as JSON)
    const figureData = [];

    document.addEventListener('DOMContentLoaded', async function() {
      // Initialize Mermaid
      mermaid.initialize({ startOnLoad: false, theme: 'default' });

      // Render Mermaid diagrams manually to catch errors
      const mermaidNodes = document.querySelectorAll('.mermaid');
      for (let i = 0; i < mermaidNodes.length; i++) {
        const el = mermaidNodes[i];
        try {
          const { svg } = await mermaid.render('mermaid-svg-' + i, el.textContent);
          el.innerHTML = svg;
        } catch (e) {
          el.innerHTML = '<div class="p-4 text-red-500 text-sm">Could not render diagram.</div>';
        }
      }

      // Render Plotly figures
      figureData.forEach(function(fig) {
        if (fig.figure_type !== 'plotly') return;
        const el = document.getElementById('plotly-' + fig.figure_id);
        if (!el) return;
        try {
          const figJson = typeof fig.data === 'string' ? JSON.parse(fig.data) : fig.data;
          Plotly.react(el, figJson.data || [], figJson.layout || {}, { responsive: true });
        } catch (e) {
          el.innerHTML = '<div class="p-4 text-red-500 text-sm">Could not render figure: ' + e.message + '</div>';
        }
      });

      // Section nav highlighting via IntersectionObserver
      const sections = document.querySelectorAll('.section-card');
      const navLinks = document.querySelectorAll('.nav-link');

      const observer = new IntersectionObserver(
        function(entries) {
          entries.forEach(function(entry) {
            if (entry.isIntersecting) {
              navLinks.forEach(function(link) { link.classList.remove('active'); });
              const id = entry.target.id.replace('section-', '');
              const activeLink = document.querySelector('.nav-link[data-section="' + id + '"]');
              if (activeLink) activeLink.classList.add('active');
            }
          });
        },
        { rootMargin: '-20% 0px -60% 0px' }
      );

      sections.forEach(function(section) { observer.observe(section); });
    });

    // Reading progress
    function updateProgress() {
      const scrollTop = window.scrollY;
      const docHeight = document.documentElement.scrollHeight - window.innerHeight;
      const pct = docHeight > 0 ? Math.round((scrollTop / docHeight) * 100) : 0;
      document.getElementById('progress-bar').style.width = pct + '%';
      document.getElementById('lesson-progress').style.width = pct + '%';
      document.getElementById('progress-pct').textContent = pct;
    }
  </script>

</body>
</html>